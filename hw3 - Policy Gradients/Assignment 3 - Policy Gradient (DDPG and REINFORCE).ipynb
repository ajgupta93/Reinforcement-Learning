{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Policy Gradients (DDPG and REINFORCE)\n",
    "\n",
    "Name:\n",
    "\n",
    "ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This exercise requires you to solve various continous control problems in OpenAI-Gym.  \n",
    "\n",
    "DDPG is policy gradient actor critic method for continous control which is off policy. It tackles the curse of dimensionality / loss of performance faced when discretizing a continous action domain. DDPG uses similiar \"tricks\" as DQN to improve the stability of training, including a replay buffer and target networks.\n",
    "\n",
    "Furthermore, you will implement REINFORCE for discrete and continous environments, and as a bonus compare the sample efficiency and performance with DQN and DDPG.\n",
    "\n",
    "\n",
    "### DDPG paper: https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:\n",
    "<img src=\"inverted_pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Pendulum-v0 environment:\n",
    "<img src=\"pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"half_cheetah.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment for Actor Critic\n",
    "- inline plotting\n",
    "- gym\n",
    "- directory for logging videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "#environment\n",
    "import gym\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an action normalization class:\n",
    "To train across various environments, it is useful to normalize action inputs and outputs between [-1, 1]. This class should take in actions and implement forward and reverse functions to map actions between [-1, 1] and [action_space.low, action_space.high].\n",
    "\n",
    "Using the following gym wrapper, implement this class.\n",
    "- https://github.com/openai/gym/blob/78c416ef7bc829ce55b404b6604641ba0cf47d10/gym/core.py\n",
    "- i.e. we are overriding the outputs scale of actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def _action(self, action):\n",
    "        #tanh outputs (-1,1) from tanh, need to be [action_space.low, action_space.high]\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        #reverse of that above\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up gym environment\n",
    "The code below does the following for you:\n",
    "- Wrap environment, log videos, setup CUDA variables (if GPU is available)\n",
    "- Record action and observation space dimensions\n",
    "- Fix random seed for determinisitic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-18 07:16:48,017] Making new env: Pendulum-v0\n",
      "[2018-05-18 07:16:48,021] Clearing 14 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "VISUALIZE = True\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 200\n",
    "NUM_EPISODES = 12000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "logging_interval = 50\n",
    "animate_interval = logging_interval * 5\n",
    "\n",
    "# Environments to be tested on\n",
    "#env_name = 'InvertedPendulum-v1'\n",
    "env_name = 'Pendulum-v0'\n",
    "#env_name = 'HalfCheetah-v1' \n",
    "logdir='./DDPG/'+env_name\n",
    "\n",
    "# wrap gym to save videos\n",
    "env = NormalizeAction(gym.make(env_name))\n",
    "\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "#env._max_episode_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "def to_numpy(var):\n",
    "    return var.cpu().data.numpy() if use_cuda else var.data.numpy()\n",
    "\n",
    "def to_tensor(x, volatile=False, requires_grad=True):\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = Variable(x, requires_grad=requires_grad).type(dtype=Tensor)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate your understanding of the simulation:\n",
    "For the environments mentioned above ('Pendulum-v0', 'HalfCheetah-v2', 'InvertedPendulum-v2'),\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space\n",
    "- when is the environment considered \"solved\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a weight syncing function\n",
    "In contrast to DQN, DDPG uses soft weight sychronization. At each time step following training, the actor and critic target network weights are updated to track the rollout networks. \n",
    "- target_network.weights <= target_network.weights \\* (1 - tau) + source_network.weights \\* (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Replay class that includes all the functionality of a replay buffer\n",
    "DDPG is an off policy actor-critic method and an identical replay buffer to that used for the previous assignment is applicable here as well (do not include the generate_minibatch method in your Replay class this time). Like before, your constructor for Replay should create an initial buffer of size 1000 when you instantiate it.\n",
    "\n",
    "The replay buffer should kept to some maximum size (60000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Replay(object):\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.capacity = max_size\n",
    "        self.position = 0\n",
    "        self.initialize(init_length=1000)\n",
    "        \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (np.asarray(state), action, reward,\\\n",
    "                                      np.asarray(next_state), done)\n",
    "        self.position = (self.position+1)%self.capacity\n",
    "    \n",
    "    def initialize(self, init_length, env=env):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.uniform(-1.0, 1.0, size=env.action_space.shape)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            self.add_experience(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                if len(self.buffer)>=init_length:\n",
    "                    break\n",
    "            else:\n",
    "                state = next_state\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminates = []\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        for state, action, reward, next_state, done in samples:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            terminates.append(done)\n",
    "        \n",
    "        states = np.array(states, dtype=np.float).reshape(batch_size,-1)\n",
    "        actions = np.array(actions, dtype=np.float).reshape(batch_size,-1)\n",
    "        rewards = np.array(rewards, dtype=np.float).reshape(batch_size,-1)\n",
    "        next_states = np.array(next_states, dtype=np.float).reshape(batch_size,-1)\n",
    "        terminates = np.array(terminates, dtype=np.float).reshape(batch_size,-1)\n",
    "        return states, actions, rewards, next_states, terminates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an Ornstein Uhlenbeck process class for exploration noise\n",
    "The proccess is described here:\n",
    "- https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process\n",
    "- http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "You should implement:\n",
    "- a step / sample method\n",
    "- reset method\n",
    "\n",
    "Use theta = 0.15, mu = 0, sigma = 0.3, dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess(object):\n",
    "    def __init__(self, dimension, num_steps, theta=0.25, mu=0.0, sigma=0.05, dt=0.01):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x = np.zeros((dimension,))\n",
    "        self.iter = 0\n",
    "        self.num_steps = num_steps\n",
    "        self.dimension = dimension\n",
    "        self.min_epsilon = 0.01 # minimum exploration probability\n",
    "        self.epsilon = 1.0\n",
    "        self.decay_rate = 5.0/num_steps # exponential decay rate for exploration prob\n",
    "    \n",
    "    def sample(self):\n",
    "        self.x = self.x + self.theta*(self.mu-self.x)*self.dt + \\\n",
    "                                       self.sigma*np.sqrt(self.dt)*np.random.normal(size=self.dimension)\n",
    "        return self.epsilon*self.x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 0*self.x\n",
    "        self.iter += 1\n",
    "        self.epsilon = self.min_epsilon + (1.0 - self.min_epsilon)*np.exp(-self.decay_rate*self.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f323698f320>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = OrnsteinUhlenbeckProcess(dimension=1,num_steps=1000)\n",
    "y = np.zeros((1000))\n",
    "for i in range(1000):\n",
    "    y[i] = noise.sample()\n",
    "plt.plot(range(1000),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture for actor and critic networks\n",
    "\n",
    "\n",
    "#### Actor\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: Tanh\n",
    "\n",
    "- hidden_state sizes: 400\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers\n",
    "\n",
    "- weight initialization: normal distribution with small variance. \n",
    "\n",
    "#### Critic\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: None\n",
    "\n",
    "- hidden_state sizes: 300, 300 + action size\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers prior to the action input\n",
    "\n",
    "- weight initialization: normal distribution with small variance.\n",
    "\n",
    "Good baselines can be found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# actor model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 400 units per layer, tanh output to bound outputs between -1 and 1\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).normal_(0.0, v)\n",
    "\n",
    "class actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(400)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.fc3 = nn.Linear(400, output_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self, init_w=10e-3):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.normal_(0, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        out = self.fc1(state)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "        action = F.tanh(self.fc3(out))\n",
    "        return action\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# critic model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 300 units per layer, ouputs rewards therefore unbounded\n",
    "# Action not to be included until 2nd layer of critic (from paper). Make sure \n",
    "# to formulate your critic.forward() accordingly\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, output_size):\n",
    "        super(critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 300)\n",
    "        #self.bn1 = nn.BatchNorm1d(300)\n",
    "        self.fc2 = nn.Linear(300 + action_size, 300)\n",
    "        self.fc3 = nn.Linear(300, output_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self, init_w=10e-3):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.normal_(0, 3e-4)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        out = self.fc1(state)\n",
    "        #out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.relu(self.fc2(torch.cat([out,action],1)))\n",
    "        qvalue = self.fc3(out)\n",
    "        return qvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG class to encapsulate definition, rollouts, and training\n",
    "\n",
    "- gamma = 0.99\n",
    "\n",
    "- actor_lr = 1e-4\n",
    "\n",
    "- critic_lr = 1e-3\n",
    "\n",
    "- critic l2 regularization = 1e-2\n",
    "\n",
    "- noise decay\n",
    "\n",
    "- noise class\n",
    "\n",
    "- batch_size = 128\n",
    "\n",
    "- optimizer: Adam\n",
    "\n",
    "- loss (critic): mse\n",
    "\n",
    "Furthermore, you can experiment with action versus parameter space noise. The standard implimentation works with action space noise, howeve parameter space noise has shown to produce excellent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = env.observation_space.shape[0]\n",
    "ACT_DIM = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = 0.99, alpha_decay=0.93, batch_size = 64):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        \n",
    "        # actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor.cuda()\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target.cuda()\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # critic\n",
    "        self.critic = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic.cuda()\n",
    "        self.critic_target = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target.cuda()\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = critic_lr, weight_decay=1e-2)\n",
    "        \n",
    "        # learning rate scheduler\n",
    "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer_actor, step_size=5000, gamma=alpha_decay)\n",
    "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer_critic, step_size=5000, gamma=alpha_decay)\n",
    "        \n",
    "        \n",
    "        # critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim, num_steps = NUM_EPISODES)\n",
    "\n",
    "        # replay buffer \n",
    "        self.replayBuffer = Replay(60000)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "     \n",
    "        # sample from Replay\n",
    "        states, actions, rewards, next_states, terminates = self.replayBuffer.sample(self.batch_size)\n",
    "\n",
    "        # update critic (create target for Q function)\n",
    "        target_qvalues = self.critic_target(to_tensor(next_states, volatile=True),\\\n",
    "                                           self.actor_target(to_tensor(next_states, volatile=True)))\n",
    "        #target_qvalues.volatile = False\n",
    "        y = to_numpy(to_tensor(rewards) +\\\n",
    "            self.gamma*to_tensor(1-terminates)*target_qvalues)\n",
    "\n",
    "        q_values = self.critic(to_tensor(states),\n",
    "                               to_tensor(actions))\n",
    "        qvalue_loss = self.critic_loss(q_values, to_tensor(y, requires_grad=False))\n",
    "        \n",
    "               \n",
    "        # critic optimizer and backprop step (feed in target and predicted values to self.critic_loss)\n",
    "        self.critic.zero_grad()\n",
    "        qvalue_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.scheduler_critic.step()\n",
    "        \n",
    "\n",
    "        # update actor (formulate the loss wrt which actor is updated)\n",
    "        policy_loss = -self.critic(to_tensor(states),\\\n",
    "                                 self.actor(to_tensor(states)))\n",
    "        policy_loss = policy_loss.mean()\n",
    "        \n",
    "\n",
    "        # actor optimizer and backprop step (loss_actor.backward())\n",
    "        self.actor.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.scheduler_actor.step()\n",
    "        \n",
    "        # sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of your DDPG object\n",
    "- Print network architectures, confirm they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-18 07:16:51,528] Starting new video recorder writing to /datasets/home/09/409/ee276cae/Reinforcement-Learning/hw3 - Policy Gradients/DDPG/Pendulum-v0/openaigym.video.3.221.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor(\n",
      "  (fc1): Linear(in_features=3, out_features=400, bias=True)\n",
      "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (bn2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n",
      "critic(\n",
      "  (fc1): Linear(in_features=3, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=301, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPG(obs_dim=OBS_DIM, act_dim=ACT_DIM, gamma=GAMMA, batch_size=BATCH_SIZE)\n",
    "print(ddpg.actor)\n",
    "print(ddpg.critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPG on different environments\n",
    "Early stopping conditions:\n",
    "- avg_val > 500 for \"InvertedPendulum\" \n",
    "- avg_val > -150 for \"Pendulum\" \n",
    "- avg_val > 1500 for \"HalfCheetah\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -1370.5738113640298\n",
      "Average value: -68.5286905682015 for episode: 0\n",
      "400 -1184.9885030854368\n",
      "Average value: -124.35168119406326 for episode: 1\n",
      "600 -1536.0860384473199\n",
      "Average value: -194.9383990567261 for episode: 2\n",
      "800 -1405.9003905013572\n",
      "Average value: -255.48649862895763 for episode: 3\n",
      "1000 -1546.9180518766912\n",
      "Average value: -320.0580762913443 for episode: 4\n",
      "1200 -1397.2081432807818\n",
      "Average value: -373.9155796408162 for episode: 5\n",
      "1400 -1300.7063079265288\n",
      "Average value: -420.2551160551018 for episode: 6\n",
      "1600 -1491.302146887883\n",
      "Average value: -473.8074675967408 for episode: 7\n",
      "1800 -1525.2012523294475\n",
      "Average value: -526.3771568333761 for episode: 8\n",
      "2000 -1451.8703934363787\n",
      "Average value: -572.6518186635262 for episode: 9\n",
      "2200 -1419.7598199078557\n",
      "Average value: -615.0072187257427 for episode: 10\n",
      "2400 -1480.669271671355\n",
      "Average value: -658.2903213730233 for episode: 11\n",
      "2600 -1388.8256611258932\n",
      "Average value: -694.8170883606667 for episode: 12\n",
      "2800 -1423.6297673928368\n",
      "Average value: -731.2577223122753 for episode: 13\n",
      "3000 -1235.8837310340248\n",
      "Average value: -756.4890227483628 for episode: 14\n",
      "3200 -1407.9539522426592\n",
      "Average value: -789.0622692230777 for episode: 15\n",
      "3400 -1508.0677241747617\n",
      "Average value: -825.0125419706618 for episode: 16\n",
      "3600 -1493.4232288270084\n",
      "Average value: -858.4330763134792 for episode: 17\n",
      "3800 -1426.356548822649\n",
      "Average value: -886.8292499389377 for episode: 18\n",
      "4000 -1360.8849447498167\n",
      "Average value: -910.5320346794815 for episode: 19\n",
      "4200 -1519.7055914921787\n",
      "Average value: -940.9907125201164 for episode: 20\n",
      "4400 -1479.0686629480329\n",
      "Average value: -967.8946100415121 for episode: 21\n",
      "4600 -1447.8087890508107\n",
      "Average value: -991.890318991977 for episode: 22\n",
      "4800 -1502.6750196501775\n",
      "Average value: -1017.429554024887 for episode: 23\n",
      "5000 -1609.2532257785515\n",
      "Average value: -1047.0207376125702 for episode: 24\n",
      "5200 -1674.9880618778157\n",
      "Average value: -1078.4191038258323 for episode: 25\n",
      "5400 -1570.3739099004172\n",
      "Average value: -1103.0168441295616 for episode: 26\n",
      "5600 -1153.909308253415\n",
      "Average value: -1105.5614673357543 for episode: 27\n",
      "5800 -1591.1901084736576\n",
      "Average value: -1129.8428993926493 for episode: 28\n",
      "6000 -1581.1487454364242\n",
      "Average value: -1152.408191694838 for episode: 29\n",
      "6200 -1249.0285649091938\n",
      "Average value: -1157.2392103555558 for episode: 30\n",
      "6400 -1173.71611998767\n",
      "Average value: -1158.0630558371615 for episode: 31\n",
      "6600 -1176.962094306858\n",
      "Average value: -1159.0080077606465 for episode: 32\n",
      "6800 -1380.0671139542708\n",
      "Average value: -1170.0609630703275 for episode: 33\n",
      "7000 -1077.8374473466404\n",
      "Average value: -1165.4497872841432 for episode: 34\n",
      "7200 -1008.2946255222405\n",
      "Average value: -1157.592029196048 for episode: 35\n",
      "7400 -1492.261037907966\n",
      "Average value: -1174.325479631644 for episode: 36\n",
      "7600 -1249.5965177032808\n",
      "Average value: -1178.0890315352258 for episode: 37\n",
      "7800 -995.3188547058024\n",
      "Average value: -1168.9505226937545 for episode: 38\n",
      "8000 -1061.8927287144368\n",
      "Average value: -1163.5976329947887 for episode: 39\n",
      "8200 -941.6569117498827\n",
      "Average value: -1152.5005969325434 for episode: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-18 07:20:01,694] Starting new video recorder writing to /datasets/home/09/409/ee276cae/Reinforcement-Learning/hw3 - Policy Gradients/DDPG/Pendulum-v0/openaigym.video.3.221.video000050.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400 -972.1382964280394\n",
      "Average value: -1143.482481907318 for episode: 41\n",
      "8600 -789.0923074537203\n",
      "Average value: -1125.7629731846382 for episode: 42\n",
      "8800 -662.6023695940953\n",
      "Average value: -1102.6049430051112 for episode: 43\n",
      "9000 -672.122781687171\n",
      "Average value: -1081.0808349392141 for episode: 44\n",
      "9200 -692.9106598827914\n",
      "Average value: -1061.6723261863929 for episode: 45\n",
      "9400 -1453.0874319119202\n",
      "Average value: -1081.2430814726692 for episode: 46\n",
      "9600 -685.4092838046939\n",
      "Average value: -1061.4513915892703 for episode: 47\n",
      "9800 -916.7569265653698\n",
      "Average value: -1054.2166683380754 for episode: 48\n",
      "10000 -1285.5004225887149\n",
      "Average value: -1065.7808560506073 for episode: 49\n",
      "10200 -990.2551969628896\n",
      "Average value: -1062.0045730962215 for episode: 50\n",
      "10400 -1053.4107163178437\n",
      "Average value: -1061.5748802573025 for episode: 51\n",
      "10600 -983.4072064908702\n",
      "Average value: -1057.6664965689808 for episode: 52\n",
      "10800 -1145.58948388021\n",
      "Average value: -1062.0626459345422 for episode: 53\n",
      "11000 -1224.0166477391276\n",
      "Average value: -1070.1603460247713 for episode: 54\n",
      "11200 -1025.9422184179514\n",
      "Average value: -1067.9494396444302 for episode: 55\n",
      "11400 -801.4717949925492\n",
      "Average value: -1054.625557411836 for episode: 56\n",
      "11600 -1117.889152571405\n",
      "Average value: -1057.7887371698146 for episode: 57\n",
      "11800 -1497.0916664250885\n",
      "Average value: -1079.7538836325782 for episode: 58\n",
      "12000 -1230.3900879066452\n",
      "Average value: -1087.2856938462814 for episode: 59\n",
      "12200 -140.05160046649422\n",
      "Average value: -1039.923989177292 for episode: 60\n",
      "12400 -1291.3957602413052\n",
      "Average value: -1052.4975777304926 for episode: 61\n",
      "12600 -1501.8655346296825\n",
      "Average value: -1074.965975575452 for episode: 62\n",
      "12800 -4.029069522303538\n",
      "Average value: -1021.4191302727944 for episode: 63\n",
      "13000 -272.72464080085757\n",
      "Average value: -983.9844057991975 for episode: 64\n",
      "13200 -274.64538954167926\n",
      "Average value: -948.5174549863215 for episode: 65\n",
      "13400 -11.663263598678858\n",
      "Average value: -901.6747454169393 for episode: 66\n",
      "13600 -145.47622859615944\n",
      "Average value: -863.8648195759004 for episode: 67\n",
      "13800 -1119.3264890669313\n",
      "Average value: -876.6379030504519 for episode: 68\n",
      "14000 -133.31973620497575\n",
      "Average value: -839.471994708178 for episode: 69\n",
      "14200 -139.79561066621102\n",
      "Average value: -804.4881755060796 for episode: 70\n",
      "14400 -790.7423970485077\n",
      "Average value: -803.800886583201 for episode: 71\n",
      "14600 -139.4815252628694\n",
      "Average value: -770.5849185171844 for episode: 72\n",
      "14800 -268.35644961695476\n",
      "Average value: -745.4734950721728 for episode: 73\n",
      "15000 -274.6062832071577\n",
      "Average value: -721.930134478922 for episode: 74\n",
      "15200 -374.3837044780658\n",
      "Average value: -704.5528129788792 for episode: 75\n",
      "15400 -135.57516769091728\n",
      "Average value: -676.1039307144811 for episode: 76\n",
      "15600 -270.81934298146916\n",
      "Average value: -655.8397013278304 for episode: 77\n",
      "15800 -279.83001113749583\n",
      "Average value: -637.0392168183137 for episode: 78\n",
      "16000 -140.73003305587318\n",
      "Average value: -612.2237576301916 for episode: 79\n",
      "16200 -395.7709562124878\n",
      "Average value: -601.4011175593064 for episode: 80\n",
      "16400 -140.15750896903188\n",
      "Average value: -578.3389371297926 for episode: 81\n",
      "16600 -163.8800953081662\n",
      "Average value: -557.6159950387113 for episode: 82\n",
      "16800 -631.3577999342074\n",
      "Average value: -561.3030852834861 for episode: 83\n",
      "17000 -262.49543285477273\n",
      "Average value: -546.3627026620504 for episode: 84\n",
      "17200 -146.65419827058503\n",
      "Average value: -526.3772774424771 for episode: 85\n",
      "17400 -140.91651021245312\n",
      "Average value: -507.10423908097584 for episode: 86\n",
      "17600 -139.44232793391726\n",
      "Average value: -488.7211435236229 for episode: 87\n",
      "17800 -270.84344330138134\n",
      "Average value: -477.8272585125108 for episode: 88\n",
      "18000 -137.2703118795628\n",
      "Average value: -460.7994111808634 for episode: 89\n",
      "18200 -13.217961407562367\n",
      "Average value: -438.4203386921983 for episode: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-18 07:21:09,466] Starting new video recorder writing to /datasets/home/09/409/ee276cae/Reinforcement-Learning/hw3 - Policy Gradients/DDPG/Pendulum-v0/openaigym.video.3.221.video000100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18400 -143.81588806044218\n",
      "Average value: -423.69011616061044 for episode: 91\n",
      "18600 -135.87767574656374\n",
      "Average value: -409.29949413990806 for episode: 92\n",
      "18800 -145.56195676004157\n",
      "Average value: -396.1126172709147 for episode: 93\n",
      "19000 -10.156930862120726\n",
      "Average value: -376.814832950475 for episode: 94\n",
      "19200 -284.7749306836489\n",
      "Average value: -372.2128378371337 for episode: 95\n",
      "19400 -137.1466899229552\n",
      "Average value: -360.45953044142476 for episode: 96\n",
      "19600 -138.11035059004368\n",
      "Average value: -349.3420714488557 for episode: 97\n",
      "19800 -137.87667034778443\n",
      "Average value: -338.7688013938021 for episode: 98\n",
      "20000 -17.71542166983977\n",
      "Average value: -322.716132407604 for episode: 99\n",
      "20200 -136.00971358125855\n",
      "Average value: -313.38081146628673 for episode: 100\n",
      "20400 -140.10478657599413\n",
      "Average value: -304.71701022177206 for episode: 101\n",
      "20600 -133.16841545514205\n",
      "Average value: -296.1395804834405 for episode: 102\n",
      "20800 -138.04858121904755\n",
      "Average value: -288.23503052022085 for episode: 103\n",
      "21000 -251.59449648511622\n",
      "Average value: -286.4030038184656 for episode: 104\n",
      "21200 -130.25664923060685\n",
      "Average value: -278.5956860890727 for episode: 105\n",
      "21400 -4.02319674475251\n",
      "Average value: -264.8670616218567 for episode: 106\n",
      "21600 -131.24266841204246\n",
      "Average value: -258.185841961366 for episode: 107\n",
      "21800 -11.842779270169784\n",
      "Average value: -245.8686888268062 for episode: 108\n",
      "22000 -134.45286852980706\n",
      "Average value: -240.29789781195623 for episode: 109\n",
      "22200 -129.7000009035629\n",
      "Average value: -234.76800296653656 for episode: 110\n",
      "22400 -240.73525294481914\n",
      "Average value: -235.06636546545067 for episode: 111\n",
      "22600 -141.86058317943093\n",
      "Average value: -230.40607635114966 for episode: 112\n",
      "22800 -2.059411639939052\n",
      "Average value: -218.9887431155891 for episode: 113\n",
      "23000 -125.09768482725357\n",
      "Average value: -214.2941902011723 for episode: 114\n",
      "23200 -139.2810628416312\n",
      "Average value: -210.54353383319526 for episode: 115\n",
      "23400 -130.64027110866627\n",
      "Average value: -206.5483706969688 for episode: 116\n",
      "23600 -129.02553847987178\n",
      "Average value: -202.67222908611393 for episode: 117\n",
      "23800 -127.4540939427579\n",
      "Average value: -198.9113223289461 for episode: 118\n",
      "24000 -240.53079309216557\n",
      "Average value: -200.9922958671071 for episode: 119\n",
      "24200 -0.27176181195052485\n",
      "Average value: -190.95626916434924 for episode: 120\n",
      "24400 -125.13562670617172\n",
      "Average value: -187.66523704144035 for episode: 121\n",
      "24600 -251.57500492933974\n",
      "Average value: -190.86072543583532 for episode: 122\n",
      "24800 -131.3911142118188\n",
      "Average value: -187.8872448746345 for episode: 123\n",
      "25000 -133.92276818449795\n",
      "Average value: -185.18902104012764 for episode: 124\n",
      "25200 -118.95182567117398\n",
      "Average value: -181.87716127167994 for episode: 125\n",
      "25400 -125.47089349113733\n",
      "Average value: -179.0568478826528 for episode: 126\n",
      "25600 -121.72224316719316\n",
      "Average value: -176.1901176468798 for episode: 127\n",
      "25800 -122.78665817157085\n",
      "Average value: -173.51994467311437 for episode: 128\n",
      "26000 -341.44904883979007\n",
      "Average value: -181.91639988144814 for episode: 129\n",
      "26200 -123.13437133464731\n",
      "Average value: -178.97729845410808 for episode: 130\n",
      "26400 -125.56680010582271\n",
      "Average value: -176.3067735366938 for episode: 131\n",
      "26600 -0.955793535112935\n",
      "Average value: -167.53922453661477 for episode: 132\n",
      "26800 -0.6968614815532918\n",
      "Average value: -159.19710638386167 for episode: 133\n",
      "27000 -132.39372606550873\n",
      "Average value: -157.85693736794403 for episode: 134\n",
      "27200 -130.5393488276222\n",
      "Average value: -156.49105794092793 for episode: 135\n",
      "27400 -129.8991044551078\n",
      "Average value: -155.1614602666369 for episode: 136\n",
      "27600 -135.26890982850043\n",
      "Average value: -154.16683274473007 for episode: 137\n",
      "27800 -128.9201538420245\n",
      "Average value: -152.9044987995948 for episode: 138\n",
      "28000 -120.66496966096383\n",
      "Average value: -151.29252234266323 for episode: 139\n",
      "28200 -131.5926002885541\n",
      "Average value: -150.30752623995775 for episode: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-18 07:22:14,228] Starting new video recorder writing to /datasets/home/09/409/ee276cae/Reinforcement-Learning/hw3 - Policy Gradients/DDPG/Pendulum-v0/openaigym.video.3.221.video000150.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28400 -124.43970876202759\n",
      "Average value: -149.01413536606123 for episode: 141\n",
      "28600 -369.3279018214439\n",
      "Average value: -160.02982368883036 for episode: 142\n",
      "28800 -131.89869240473456\n",
      "Average value: -158.62326712462556 for episode: 143\n",
      "29000 -134.81691260620133\n",
      "Average value: -157.43294939870435 for episode: 144\n",
      "29200 -234.97013804091574\n",
      "Average value: -161.30980883081492 for episode: 145\n",
      "29400 -118.4357039178567\n",
      "Average value: -159.16610358516698 for episode: 146\n",
      "29600 -122.67089451982228\n",
      "Average value: -157.34134313189975 for episode: 147\n",
      "29800 -0.6420312248829001\n",
      "Average value: -149.50637753654888 for episode: 148\n",
      "30000 -133.95849555472827\n",
      "Average value: -148.72898343745783 for episode: 149\n",
      "30200 -248.30474679107516\n",
      "Average value: -153.70777160513867 for episode: 150\n",
      "30400 -3.3519829033852164\n"
     ]
    }
   ],
   "source": [
    "avg_val = 0\n",
    "\n",
    "#for plotting\n",
    "running_rewards_ddpg = []\n",
    "step_list_ddpg = []\n",
    "step_counter = 0\n",
    "\n",
    "# set term_condition for early stopping according to environment being used\n",
    "term_condition = -150 # Pendulum\n",
    "\n",
    "reached = False\n",
    "\n",
    "for itr in range(NUM_EPISODES):\n",
    "    state = env.reset() # get initial state\n",
    "    animate_this_episode = (itr % animate_interval == 0) and VISUALIZE\n",
    "    ddpg.noise.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "        # use actor to get action, add ddpg.noise.step() to action\n",
    "        # remember to put NN in eval mode while testing (to deal with BatchNorm layers) and put it back \n",
    "        # to train mode after you're done getting the action\n",
    "        ddpg.actor.eval()\n",
    "        state = state.reshape(1,-1)\n",
    "        noise = ddpg.noise.sample()\n",
    "        action = to_numpy(ddpg.actor(to_tensor(state))).reshape(-1,) + noise\n",
    "        if not reached:\n",
    "            ddpg.actor.train()\n",
    "                \n",
    "        # step action, get next state, reward, done (keep track of total_reward)\n",
    "        # populate ddpg.replayBuffer\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ddpg.replayBuffer.add_experience(state.ravel(), action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "\n",
    "        ddpg.train()\n",
    "        step_counter += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        step+=1\n",
    "    \n",
    "    print(step_counter, total_reward)\n",
    "\n",
    "    if avg_val > term_condition and itr>10:\n",
    "        reached = True\n",
    "    \n",
    "    if reached and itr%logging_interval==1:\n",
    "        break\n",
    "\n",
    "    running_rewards_ddpg.append(total_reward) # return of this episode\n",
    "    step_list_ddpg.append(step_counter)\n",
    "\n",
    "    avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "    print(\"Average value: {} for episode: {}\".format(avg_val,itr))\n",
    "    itr+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over multiple training runs \n",
    "This is provided to generate and plot results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "In this section you will implement REINFORCE, with modifications for batch training. It will be for use on both discrete and continous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Parametrization\n",
    "\n",
    "Define a MLP which outputs a distribution over the action preferences given input state. For the discrete case, the MLP outputs the likelihood of each action (softmax) while for the continuous case, the output is the mean and standard deviation parametrizing the normal distribution from which the action is sampled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Policy parametrizing model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 1 or 2 hidden layers with a small number of units per layer (similar to DQN)\n",
    "# use ReLU for hidden layer activations\n",
    "# softmax as activation for output if discrete actions, linear for continuous control\n",
    "# for the continuous case, output_dim=2*act_dim (each act_dim gets a mean and std_dev)\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    __init__(self, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that samples an action from the policy distribtion parameters obtained as output of the MLP. The function should return the action and the log-probability (log_odds) of taking that action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(logit, discrete):\n",
    "    # logit is the output of the softmax/linear layer\n",
    "    # discrete is a flag for the environment type\n",
    "    # Hint: use Categorical and Normal from torch.distributions to sample action and get the log-probability\n",
    "    # Note that log_probability in this case translates to ln(\\pi(a|s)) \n",
    "           \n",
    "    return action, log_odds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function update_policy that defines the loss function and updates the MLP according to the REINFORCE update rule (ref. slide 24 of Lec 7 or page 330 of Sutton and Barto (2018)). The update algorithm to be used below is slightly different: instead of updating the network at every time-step, we take the gradient of the loss averaged over a batch of timesteps (this is to make SGD more stable). We also use a baseline to reduce variance. \n",
    "\n",
    "The discount factor is set as 1 here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(paths, net):\n",
    "    # paths: a list of paths (complete episodes, used to calculate return at each time step)\n",
    "    # net: MLP object\n",
    "\n",
    "    \n",
    "    num_paths = len(paths)\n",
    "    rew_cums = []\n",
    "    log_odds = []\n",
    "    \n",
    "    for path in paths:\n",
    "         # rew_cums should record return at each time step for each path \n",
    "         \n",
    "         # log_odds should record log_odds obtained at each timestep of path\n",
    "         \n",
    "         # calculated as \"reward to go\"\n",
    "            \n",
    "    rew_cums = (rew_cums - rew_cums.mean()) / (rew_cums.std() + 1e-5) # create baseline\n",
    "    \n",
    "    # make log_odds, rew_cums each a vector\n",
    "        \n",
    "    # calculate policy loss and average over paths\n",
    "    \n",
    "    # take optimizer step\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment and instantiate objects. Your algorithm is to be tested on one discrete and two continuous environments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Environment\n",
    "\n",
    "#discrete environment:\n",
    "#env_name='CartPole-v0'\n",
    "\n",
    "#continous environments:\n",
    "#env_name='InvertedPendulum-v2'\n",
    "#env_name = 'HalfCheetah-v2'\n",
    "\n",
    "env_name='InvertedPendulum-v2'\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "visualize = False\n",
    "animate=visualize\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_path_length=None\n",
    "\n",
    "\n",
    "# Set random seeds\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Saving parameters\n",
    "logdir='./REINFORCE/'\n",
    "\n",
    "\n",
    "if visualize:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%animate_interval==0)\n",
    "env._max_episode_steps = min_timesteps_per_batch\n",
    "\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Get observation and action space dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Make network object (remember to pass in appropriate flags for the type of action space in use)\n",
    "# net = mlp(*args)\n",
    "\n",
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run REINFORCE\n",
    "\n",
    "Run REINFORCE for CartPole, InvertedPendulum, and HalfCheetah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000 \n",
    "min_timesteps_per_batch = 2000  # sets the batch size for updating network\n",
    "avg_reward = 0\n",
    "avg_rewards = []\n",
    "step_list_reinforce = []\n",
    "total_steps = 0\n",
    "episodes = 0\n",
    "\n",
    "for itr in range(n_iter): # loop for number of optimization steps\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    \n",
    "    while True: # loop to get enough timesteps in this batch --> if episode ends this loop will restart till steps reaches limit\n",
    "        ob = env.reset()\n",
    "        obs, acs, rews, log_odds = [], [], [], [] \n",
    "       \n",
    "        while True: # loop for episode inside batch\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            # get parametrized policy distribution from net using current state ob\n",
    "            \n",
    "            # sample action and get log-probability (log_odds) from distribution\n",
    "            \n",
    "            # step environment, record reward, next state\n",
    "            \n",
    "            # append to obs, acs, rewards, log_odds\n",
    "            \n",
    "            # if done, restart episode till min_timesteps_per_batch is reached\n",
    "                     \n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                episodes = episodes + 1\n",
    "                break\n",
    "                \n",
    "        path = {\"observation\" : obs, \n",
    "                \"reward\" : np.array(rews), \n",
    "                \"action\" : (acs),\n",
    "                \"log_odds\" : log_odds}\n",
    "        \n",
    "        paths.append(path)\n",
    "        \n",
    "        if steps > min_timesteps_per_batch:\n",
    "            break \n",
    "        \n",
    "    update_policy(paths, net)  # use all complete episodes (a batch of timesteps) recorded in this itr to update net\n",
    "    \n",
    "    if itr == 0:\n",
    "        avg_reward = path['reward'].sum()\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "    \n",
    "    if avg_reward > 300:\n",
    "        break\n",
    "    \n",
    "    total_steps += steps\n",
    "    avg_rewards.append(avg_reward)\n",
    "    step_list_reinforce.append(total_steps)\n",
    "    if itr % logging_interval == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "   \n",
    "      \n",
    "env.close()\n",
    "\n",
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (15% extra)\n",
    "\n",
    "Compare average returns for CartPole (discrete action space) when using REINFORCE and DQN. Since in REINFORCE we update the network after a set number of steps instead of after every episode, plot the average rewards as a function of steps rather than episodes for both DQN and REINFORCE. You will need to make minor edits to your DQN code from the previous assignment to record average returns as a function of time_steps.\n",
    "\n",
    "Similarly, compare REINFORCE with DDPG on InvertedPendulum and HalfCheetah using steps for the x-axis.\n",
    "\n",
    "You may use the example code provided below as a reference for the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import your DQN and format your average returns as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "plt.plot(step_list_ddpg, out) # or plt.plot(step_list_DQN, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(['DDPG', 'REINFORCE']) #or plt.legend(['DQN', 'REINFORCE'])\n",
    "plt.plot(step_list_reinforce, avg_rewards)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
